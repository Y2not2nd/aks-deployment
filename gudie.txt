This is an excellent starting point for a project guide, but as you requested, let's make it way more specific by adding actual, actionable steps and details that are often overlooked or assumed. I'll break down each section and inject the missing information.

🚀 ULTIMATE FOOLPROOF FULL PROJECT GUIDE — ALL STEPS, FILES, COMMANDS, EXPLANATIONS, LOCATIONS
✅ This document provides the entire end-to-end project, with every step in the correct order, exact file paths, where to run each command (PowerShell, VS Code terminal, Docker CLI, Azure Cloud Shell), edit notes, why we do each step, and completion checkpoints.

🏗 1️⃣️⃣ PROJECT PREPARATION
Tools you need on Windows:
	• ✅ Docker Desktop: Install from the official Docker website. Ensure it's running and configured for WSL 2 backend (recommended for performance). 
		○ Actual Step: Download Docker Desktop Installer.exe from https://www.docker.com/products/docker-desktop/. Run the installer, following the prompts. After installation, start Docker Desktop. Go to Settings > Resources > WSL Integration and ensure your Linux distribution (e.g., Ubuntu) is enabled.
	• ✅ Terraform (install via Chocolatey): Chocolatey is a package manager for Windows. 
		○ Actual Step 1 (Install Chocolatey if not already installed): Open PowerShell as Administrator and run: 
PowerShell

Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))

Confirm by typing Y if prompted.
		○ Actual Step 2 (Install Terraform via Chocolatey): Close and reopen PowerShell as Administrator. Then run: 
PowerShell

choco install terraform -y
		○ Verification: Open a new PowerShell window (not as Administrator) and run terraform --version. You should see the installed Terraform version.
	• ✅ Azure CLI: Command-line tools for managing Azure resources. 
		○ Actual Step: Download the MSI installer from https://learn.microsoft.com/en-us/cli/azure/install-azure-cli-windows?tabs=azure-cli. Run the installer, accepting defaults.
		○ Verification: Open PowerShell and run az --version. You should see the installed Azure CLI version.
	• ✅ Helm: Kubernetes package manager. 
		○ Actual Step 1 (Install via Chocolatey): Open PowerShell as Administrator and run: 
PowerShell

choco install kubernetes-helm -y
		○ Verification: Open a new PowerShell window (not as Administrator) and run helm version. You should see the installed Helm version.
	• ✅ kubectl: Kubernetes command-line tool. Often installed with Docker Desktop, but good to ensure it's accessible. 
		○ Actual Step 1 (Install via Chocolatey): Open PowerShell as Administrator and run: 
PowerShell

choco install kubernetes-cli -y
		○ Verification: Open a new PowerShell window (not as Administrator) and run kubectl version --client. You should see the installed kubectl client version.
	• ✅ Git (and GitHub account): Version control. 
		○ Actual Step: Download the installer from https://git-scm.com/download/win. Run the installer, accepting the recommended defaults. Ensure "Git Bash Here" is enabled.
		○ Actual Step 2 (GitHub Account): Create a free account at https://github.com/join.
		○ Verification: Open PowerShell or Git Bash and run git --version.
	• ✅ VS Code with extensions (Terraform, Kubernetes, Helm, Docker, Azure Tools): 
		○ Actual Step 1 (Install VS Code): Download from https://code.visualstudio.com/. Run the installer.
		○ Actual Step 2 (Install Extensions): Open VS Code. Go to the Extensions view (Ctrl+Shift+X or click the Extensions icon on the left sidebar). Search for and install the following: 
			§ HashiCorp Terraform
			§ Microsoft Kubernetes
			§ Helm (by Microsoft)
			§ Docker (by Microsoft)
			§ Azure Tools (by Microsoft)
			§ Pro-tip: The Azure Tools extension pack usually includes Azure CLI integration, Azure Account, Azure Resources, etc., making it easier.
Folder structure: C:/projects/aks-k8s-project
	• Actual Step: Create this folder manually: 
		○ Open File Explorer.
		○ Navigate to C:.
		○ Right-click, select New > Folder, and name it projects.
		○ Inside C:/projects, right-click, select New > Folder, and name it aks-k8s-project.
✅ Checkpoint: All tools installed, folders set up.
	• Verification: Double-check each tool's version in PowerShell and ensure the folder structure is correct.

🌍 2️⃣️⃣ INFRASTRUCTURE SETUP WITH TERRAFORM
📂 Files:
	• C:/projects/aks-k8s-project/terraform/main.tf
	• C:/projects/aks-k8s-project/terraform/variables.tf
	• C:/projects/aks-k8s-project/terraform/outputs.tf
	• C:/projects/aks-k8s-project/terraform/modules/aks/* (This implies you'll have main.tf, variables.tf, outputs.tf inside C:/projects/aks-k8s-project/terraform/modules/aks/)
⚙ Why: Automate AKS creation for repeatability and consistency. This makes your infrastructure "infrastructure as code" (IaC).
📜 Commands [VS Code Terminal, Bash]:
	• Actual Step 1 (Create terraform directory): 
Bash

mkdir -p C:/projects/aks-k8s-project/terraform/modules/aks

(Run this in PowerShell or Git Bash, as mkdir -p works well)
	• Actual Step 2 (Create Terraform files - main.tf, variables.tf, outputs.tf in the terraform directory): 
		○ Open VS Code. Go to File > Open Folder... and select C:/projects/aks-k8s-project.
		○ Inside the terraform folder, create main.tf, variables.tf, outputs.tf.
		○ C:/projects/aks-k8s-project/terraform/main.tf (Example content): 
Terraform

provider "azurerm" {
  features {}
}

resource "azurerm_resource_group" "rg" {
  name     = var.resource_group
  location = var.location
}

module "aks" {
  source              = "./modules/aks"
  resource_group_name = azurerm_resource_group.rg.name
  location            = azurerm_resource_group.rg.location
  cluster_name        = var.cluster_name
  dns_prefix          = var.dns_prefix
  node_count          = var.node_count
}

output "aks_cluster_name" {
  value = module.aks.cluster_name
}

output "kube_config" {
  value     = module.aks.kube_config_raw
  sensitive = true
}
		○ C:/projects/aks-k8s-project/terraform/variables.tf (Example content): 
Terraform

variable "resource_group" {
  description = "Name of the Azure Resource Group"
  type        = string
  default     = "aks-rg"
}

variable "location" {
  description = "Azure region for the resources"
  type        = string
  default     = "uksouth"
}

variable "cluster_name" {
  description = "Name of the AKS cluster"
  type        = string
  default     = "myaks"
}

variable "dns_prefix" {
  description = "DNS prefix for the AKS cluster"
  type        = string
  default     = "myaksdns"
}

variable "node_count" {
  description = "Number of nodes in the AKS cluster"
  type        = number
  default     = 2
}
		○ C:/projects/aks-k8s-project/terraform/outputs.tf (Example content - outputs from the main module, for now): 
Terraform

# No direct outputs from this level for now, handled by module.
# This file can be used for root-level outputs if needed.
	• Actual Step 3 (Create Terraform module files for AKS - main.tf, variables.tf, outputs.tf in the terraform/modules/aks directory): 
		○ Inside C:/projects/aks-k8s-project/terraform/modules/aks, create main.tf, variables.tf, outputs.tf.
		○ C:/projects/aks-k8s-project/terraform/modules/aks/main.tf (Example content for AKS module): 
Terraform

resource "azurerm_kubernetes_cluster" "aks" {
  name                = var.cluster_name
  location            = var.location
  resource_group_name = var.resource_group_name
  dns_prefix          = var.dns_prefix

default_node_pool {
    name       = "default"
    node_count = var.node_count
    vm_size    = "Standard_DS2_v2" # Or choose an appropriate size
  }

identity {
    type = "SystemAssigned"
  }

network_profile {
    network_plugin     = "kubenet"
    load_balancer_sku  = "standard"
  }

tags = {
    environment = "development"
    project     = "aks-k8s-project"
  }
}

output "cluster_name" {
  value = azurerm_kubernetes_cluster.aks.name
}

output "kube_config_raw" {
  value     = azurerm_kubernetes_cluster.aks.kube_config_raw
  sensitive = true
}
		○ C:/projects/aks-k8s-project/terraform/modules/aks/variables.tf (Example content for AKS module variables): 
Terraform

variable "resource_group_name" {
  description = "Name of the Azure Resource Group"
  type        = string
}

variable "location" {
  description = "Azure region for the resources"
  type        = string
}

variable "cluster_name" {
  description = "Name of the AKS cluster"
  type        = string
}

variable "dns_prefix" {
  description = "DNS prefix for the AKS cluster"
  type        = string
}

variable "node_count" {
  description = "Number of nodes in the AKS cluster"
  type        = number
}
		○ C:/projects/aks-k8s-project/terraform/modules/aks/outputs.tf (Example content for AKS module outputs): 
Terraform

output "cluster_name" {
  value = azurerm_kubernetes_cluster.aks.name
}

output "kube_config_raw" {
  value     = azurerm_kubernetes_cluster.aks.kube_config_raw
  sensitive = true
}
	• Actual Step 4 (Login to Azure): Open PowerShell and run: 
PowerShell

az login

This will open a browser window for you to log in to your Azure account.
	• Actual Step 5 (Run Terraform commands): Open VS Code terminal (Ctrl+orTerminal > New Terminal). This will likely open a PowerShell terminal by default. To use Bash, click the dropdown next to the+icon in the terminal and selectGit Bash`. 
Bash

cd C:/projects/aks-k8s-project/terraform

Then, execute the following: 
Bash

terraform init
		○ Explanation: Initializes a working directory containing Terraform configuration files. Downloads necessary provider plugins (like azurerm).
Bash

terraform plan -var="resource_group=aks-rg" -var="location=uksouth" -var="cluster_name=myaks" -var="dns_prefix=myaksdns" -var="node_count=2"
		○ Explanation: Generates an execution plan, showing what actions Terraform will take (create, modify, destroy) without actually performing them.
Bash

terraform apply -auto-approve -var="resource_group=aks-rg" -var="location=uksouth" -var="cluster_name=myaks" -var="dns_prefix=myaksdns" -var="node_count=2"
		○ Explanation: Applies the changes required to reach the desired state of the configuration. -auto-approve bypasses the interactive approval prompt.
	• ⚠️ EDIT: Replace resource_group, location, cluster_name, dns_prefix, and node_count with your desired values. Ensure location is an Azure region that supports AKS.
	• Actual Step 6 (Get AKS credentials): After terraform apply completes, you need to pull the kubeconfig. Run this in PowerShell: 
PowerShell

az aks get-credentials --resource-group aks-rg --name myaks --overwrite-cli
		○ Explanation: This command retrieves the Kubernetes configuration file for your AKS cluster and merges it with your local kubeconfig file (usually located at C:\Users\<YourUser>\.kube\config). --overwrite-cli ensures the new credentials are set as the current context.
		○ Verification: Run kubectl get nodes in your VS Code terminal (Bash). You should see your AKS nodes listed.
✅ Checkpoint: AKS cluster created, kubeconfig pulled.
	• Verification: 
		○ Go to the Azure portal and confirm the aks-rg resource group and myaks cluster exist.
		○ Run kubectl get nodes in your VS Code terminal. Ensure you see your two nodes in a Ready state.

🐳 3️⃣️⃣ DOCKERIZE THE STATIC WEBSITE
📂 Files:
	• C:/projects/aks-k8s-project/website/* (html, css, js, images)
	• C:/projects/aks-k8s-project/website/Dockerfile
⚙ Why: Package your static website into a portable Docker image, which can then be easily deployed to any Docker-compatible environment, including Kubernetes.
📜 Commands [PowerShell]:
	• Actual Step 1 (Create website directory and sample files): 
PowerShell

mkdir C:/projects/aks-k8s-project/website
		○ Open VS Code, navigate to C:/projects/aks-k8s-project/website.
		○ Create an index.html file (e.g., a simple "Hello, AKS!"). C:/projects/aks-k8s-project/website/index.html: 
HTML

<!DOCTYPE html>
<html>
<head>
    <title>Hello AKS!</title>
    <style>
        body { font-family: Arial, sans-serif; text-align: center; margin-top: 50px; }
        h1 { color: #333; }
    </style>
</head>
<body>
    <h1>Welcome to AKS!</h1>
    <p>This is a static website deployed on Azure Kubernetes Service.</p>
</body>
</html>
	• Actual Step 2 (Create Dockerfile): 
		○ In the C:/projects/aks-k8s-project/website directory, create a file named Dockerfile (no extension).
		○ C:/projects/aks-k8s-project/website/Dockerfile: 
Dockerfile

# Use an official Nginx image as a base
FROM nginx:alpine

# Copy the static website files into the Nginx default web directory
COPY . /usr/share/nginx/html

# Expose port 80 (Nginx default)
EXPOSE 80

# Start Nginx when the container launches
CMD ["nginx", "-g", "daemon off;"]
	• Actual Step 3 (Build Docker image): Open PowerShell. 
PowerShell

cd C:/projects/aks-k8s-project/website
docker build -t static-web:v1 .
		○ Explanation: docker build creates a Docker image. -t static-web:v1 tags the image with a name (static-web) and version (v1). The . indicates that the Dockerfile is in the current directory.
	• Actual Step 4 (Run Docker container locally): 
PowerShell

docker run -d -p 8080:80 static-web:v1
		○ Explanation: docker run starts a new container. -d runs the container in detached mode (in the background). -p 8080:80 maps port 8080 on your host machine to port 80 inside the container. static-web:v1 specifies the image to use.
	• Actual Step 5 (Verify local container): Open your web browser and go to http://localhost:8080. You should see your "Hello AKS!" page.
	• Actual Step 6 (Stop and remove local container): 
PowerShell

docker ps # Get the CONTAINER ID of static-web:v1
docker stop <CONTAINER_ID>
docker rm <CONTAINER_ID>
✅ Checkpoint: Local image works.
	• Verification: Confirm you can access the website at http://localhost:8080 and then successfully stop and remove the container.
📜 Push to ACR [PowerShell]:
	• Actual Step 1 (Create Azure Container Registry): 
PowerShell

az acr create -n myacrname -g aks-rg --sku Basic
		○ Explanation: Creates an Azure Container Registry (ACR) instance. -n specifies the name (must be globally unique), -g specifies the resource group, and --sku Basic selects the pricing tier (Basic is sufficient for this project).
		○ ⚠️ EDIT: Replace myacrname with a globally unique name (e.g., yourinitialsaksacr123).
	• Actual Step 2 (Log in to ACR): 
PowerShell

az acr login -n myacrname
		○ Explanation: Authenticates your Docker CLI with your Azure Container Registry, allowing you to push/pull images.
		○ ⚠️ EDIT: Use your real ACR name.
	• Actual Step 3 (Tag the Docker image for ACR): 
PowerShell

docker tag static-web:v1 myacrname.azurecr.io/static-web:v1
		○ Explanation: Tags the locally built static-web:v1 image with the full path to your ACR. This is crucial for Docker to know where to push the image.
		○ ⚠️ EDIT: Use your real ACR name.
	• Actual Step 4 (Push the Docker image to ACR): 
PowerShell

docker push myacrname.azurecr.io/static-web:v1
		○ Explanation: Uploads the tagged Docker image from your local machine to your Azure Container Registry.
		○ ⚠️ EDIT: Use your real ACR name.
✅ Checkpoint: Image pushed to ACR.
	• Verification: 
		○ Go to the Azure portal, navigate to your ACR instance (myacrname).
		○ Under Services, click on Repositories. You should see static-web listed.
		○ Click on static-web, and you should see the v1 tag.

⛵ 4️⃣ HELM DEPLOYMENT
📂 Files:
	• C:/projects/aks-k8s-project/static-web/Chart.yaml
	• C:/projects/aks-k8s-project/static-web/values.yaml
	• C:/projects/aks-k8s-project/static-web/templates/* (e.g., deployment.yaml, service.yaml, ingress.yaml if needed)
⚙ Why: Helm simplifies Kubernetes deployments by packaging applications into "charts." It allows you to define, install, and upgrade even the most complex Kubernetes applications. It manages versions and rollbacks easily.
📜 Commands [VS Code Terminal, Bash]:
	• Actual Step 1 (Create Helm chart structure): Open VS Code terminal (Bash). 
Bash

cd C:/projects/aks-k8s-project
helm create static-web
		○ Explanation: helm create generates a boilerplate Helm chart with a standard directory structure and example files.
	• Actual Step 2 (Edit Helm chart files): Open VS Code and navigate to C:/projects/aks-k8s-project/static-web. 
		○ C:/projects/aks-k8s-project/static-web/Chart.yaml: 
YAML

apiVersion: v2
name: static-web
description: A Helm chart for a simple static website
type: application
version: 0.1.0
appVersion: "1.0"
		○ C:/projects/aks-k8s-project/static-web/values.yaml: This file overrides default values in the templates. 
YAML

replicaCount: 2 # Number of website pods

image:
  repository: myacrname.azurecr.io/static-web # Your ACR image path
  pullPolicy: IfNotPresent
  tag: "v1" # The Docker image tag

service:
  type: LoadBalancer # Exposes the service externally
  port: 80

ingress:
  enabled: false # Set to true if you want to use Ingress instead of LoadBalancer
  className: ""
  annotations: {}
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi
			§ ⚠️ EDIT: Update image.repository with your actual ACR name.
		○ C:/projects/aks-k8s-project/static-web/templates/deployment.yaml: (Modify the generated deployment.yaml to use your image from values.yaml) 
YAML

apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "static-web.fullname" . }}
  labels:
    {{- include "static-web.labels" . | nindent 4 }}
spec:
  {{- if not .Values.autoscaling.enabled }}
  replicas: {{ .Values.replicaCount }}
  {{- end }}
  selector:
    matchLabels:
      {{- include "static-web.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      {{- with .Values.podAnnotations }}
      annotations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      labels:
        {{- include "static-web.selectorLabels" . | nindent 8 }}
    spec:
      {{- with .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      serviceAccountName: {{ include "static-web.serviceAccountName" . }}
      securityContext:
        {{- toYaml .Values.podSecurityContext | nindent 8 }}
      containers:
        - name: {{ .Chart.Name }}
          securityContext:
            {{- toYaml .Values.securityContext | nindent 12 }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          resources:
            {{- toYaml .Values.resources | nindent 12 }}
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
		○ C:/projects/aks-k8s-project/static-web/templates/service.yaml: (Ensure type: LoadBalancer is set as per values.yaml) 
YAML

apiVersion: v1
kind: Service
metadata:
  name: {{ include "static-web.fullname" . }}
  labels:
    {{- include "static-web.labels" . | nindent 4 }}
spec:
  type: {{ .Values.service.type }}
  ports:
    - port: {{ .Values.service.port }}
      targetPort: http
      protocol: TCP
      name: http
  selector:
    {{- include "static-web.selectorLabels" . | nindent 6 }}
	• Actual Step 3 (Add stable Helm repo): In VS Code terminal (Bash). 
Bash

helm repo add stable https://charts.helm.sh/stable
		○ Explanation: Adds the "stable" Helm chart repository, though for custom apps, it's not strictly needed for your chart, but good practice if you pull other charts.
	• Actual Step 4 (Install/Upgrade the Helm chart): 
Bash

cd C:/projects/aks-k8s-project
helm upgrade --install static-web ./static-web --namespace default
		○ Explanation: helm upgrade --install will install the chart if it doesn't exist, or upgrade it if it does. static-web is the release name, ./static-web is the path to your chart directory. --namespace default specifies the Kubernetes namespace.
	• Actual Step 5 (Get LoadBalancer IP): Wait a few minutes for the Azure Load Balancer to provision. 
Bash

kubectl get service static-web -n default
		○ Explanation: This command shows the services in the default namespace. Look for the static-web service and find its EXTERNAL-IP. It might show <pending> for a few minutes. Keep running the command until an IP address appears.
	• Actual Step 6 (Access the deployed website): Open your web browser and navigate to the EXTERNAL-IP obtained in the previous step. You should see your "Hello AKS!" website.
✅ Checkpoint: App live on AKS.
	• Verification: Confirm you can access the website via the LoadBalancer's external IP address.
	• Verification: Run kubectl get pods -n default and ensure your static-web pods are Running.

🔒 5️⃣️⃣ VAULT INTEGRATION (IN-CLUSTER HA)
📂 Files:
	• vault-policies/static-web-policy.hcl (Will create later)
	• vault-policies/static-web-role.sh (Will create later)
⚙ Why: HashiCorp Vault is a tool for securely accessing secrets. It can store sensitive data like API keys, passwords, and certificates, and provide them to applications. "In-cluster HA" means it runs highly available within your Kubernetes cluster.
📜 Commands [VS Code Terminal, Bash]:
	• Actual Step 1 (Add HashiCorp Helm repo): Open VS Code terminal (Bash). 
Bash

helm repo add hashicorp https://helm.releases.hashicorp.com
helm repo update
	• Actual Step 2 (Install Vault with HA enabled): 
Bash

helm install vault hashicorp/vault --set "server.ha.enabled=true" --set "server.ha.raft.enabled=true" --set "server.dev.enabled=false" --set "server.ingress.enabled=true" --set "server.ingress.hosts[0]=vault.yourdomain.com"
		○ Explanation: This installs Vault using its official Helm chart. 
			§ server.ha.enabled=true enables High Availability.
			§ server.ha.raft.enabled=true configures Raft storage backend for HA (simpler than Consul for small setups).
			§ server.dev.enabled=false disables the insecure dev server.
			§ server.ingress.enabled=true enables an Ingress for external access to the Vault UI/API.
			§ server.ingress.hosts[0]=vault.yourdomain.com: ⚠️ EDIT: You'll need a domain name and DNS configuration pointing to your Ingress controller's external IP for this to work properly in a real scenario. For simplicity here, you might omit ingress for now and use port-forwarding, or configure an Nginx Ingress Controller on your AKS. Let's assume for this guide you'll primarily interact via kubectl exec and eventually port-forward for UI if needed.
		○ Alternative (Simpler for initial testing, no Ingress needed): 
Bash

helm install vault hashicorp/vault --set "server.ha.enabled=true" --set "server.ha.raft.enabled=true" --set "server.dev.enabled=false"
	• Actual Step 3 (Wait for Vault pods to be ready): 
Bash

kubectl get pods -l app.kubernetes.io/name=vault

Wait until all Vault pods (vault-0, vault-1, etc.) are in a Running state.
	• Actual Step 4 (Initialize Vault): 
Bash

kubectl exec -it vault-0 -- vault operator init -key-shares=1 -key-threshold=1 -format=json > C:/projects/aks-k8s-project/vault-init.json
		○ Explanation: Initializes the Vault server. key-shares=1 -key-threshold=1 means one key is needed to unseal. -format=json outputs the keys in JSON format. The output is redirected to a file for safe keeping.
		○ ⚠️ EDIT: SAVE THE UNSEAL KEY AND ROOT TOKEN FROM vault-init.json SECURELY! These are critical for accessing and unsealing your Vault.
		○ Verification: Open C:/projects/aks-k8s-project/vault-init.json and confirm you see unseal_keys_b64 and root_token.
	• Actual Step 5 (Unseal Vault): 
		○ Open C:/projects/aks-k8s-project/vault-init.json. Copy the value of the first unseal_keys_b64 (the actual key, not the array).
		○ Run the following command, replacing <UNSEAL_KEY> with your copied key: 
Bash

kubectl exec -it vault-0 -- vault operator unseal <UNSEAL_KEY>
		○ Explanation: Unseals the Vault server, making it operational. In an HA setup, you typically unseal the active node. If you have multiple nodes and one becomes leader, unsealing the leader is usually sufficient.
		○ Verification: Run kubectl exec -it vault-0 -- vault status. It should show Sealed: false.
	• Actual Step 6 (Log in to Vault with Root Token): 
		○ Copy the root_token from C:/projects/aks-k8s-project/vault-init.json.
		○ Run: 
Bash

kubectl exec -it vault-0 -- vault login <ROOT_TOKEN>
		○ Explanation: Authenticates your Vault CLI session with the root token.
	• Actual Step 7 (Enable Kubernetes Auth Method): 
Bash

kubectl exec -it vault-0 -- vault auth enable kubernetes
		○ Explanation: Enables the Kubernetes authentication method in Vault, allowing Kubernetes service accounts to authenticate with Vault.
	• Actual Step 8 (Configure Kubernetes Auth Method): 
		○ Get the Kubernetes Host and CA Cert. 
Bash

KUBE_HOST=<span class="math-inline">\(kubectl config view \-\-raw \-\-minify \-\-flatten \-\-output 'jsonpath\=\{\.clusters\[\]\.cluster\.server\}'\)
KUBE_CA_CRT=(kubectl config view --raw --minify --flatten --output 'jsonpath={.clusters[].cluster.certificate-authority-data}') * Configure Vault:bash kubectl exec -it vault-0 -- vault write auth/kubernetes/config 
token_reviewer_jwt="$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)" 
kubernetes_host="$KUBE_HOST" 
kubernetes_ca_cert="$KUBE_CA_CRT" ``` * Explanation: Configures the Kubernetes auth method with the necessary details to communicate with your AKS cluster. The token_reviewer_jwt is used by Vault to validate service account tokens.
	• Actual Step 9 (Create a secret path): 
Bash

kubectl exec -it vault-0 -- vault secrets enable -path=kv kv-v2
		○ Explanation: Enables a Key-Value secret engine at the path kv.
	• Actual Step 10 (Write a sample secret): 
Bash

kubectl exec -it vault-0 -- vault kv put kv/static-web-secrets message="Hello from Vault!"
		○ Explanation: Stores a secret under the path kv/static-web-secrets with a key message.
	• Actual Step 11 (Create Vault policy for static-web): 
		○ Create the vault-policies directory: mkdir -p C:/projects/aks-k8s-project/vault-policies
		○ Create C:/projects/aks-k8s-project/vault-policies/static-web-policy.hcl: 
Terraform

path "kv/data/static-web-secrets" {
  capabilities = ["read"]
}
		○ Explanation: Defines a policy that grants read access to the specific secret path.
	• Actual Step 12 (Apply the Vault policy): 
Bash

kubectl exec -it vault-0 -- vault policy write static-web-policy C:/projects/aks-k8s-project/vault-policies/static-web-policy.hcl
		○ Explanation: Uploads and applies the policy to Vault.
	• Actual Step 13 (Create Kubernetes Auth Role): 
		○ Create C:/projects/aks-k8s-project/vault-policies/static-web-role.sh: 
Bash

#!/bin/bash
kubectl exec -it vault-0 -- vault write auth/kubernetes/role/static-web-role \
    bound_service_account_names=static-web-sa \
    bound_service_account_namespaces=default \
    policies=static-web-policy \
    ttl=1h
		○ Explanation: This script defines a Kubernetes authentication role in Vault. 
			§ bound_service_account_names=static-web-sa: This role can only be assumed by a Kubernetes service account named static-web-sa.
			§ bound_service_account_namespaces=default: The service account must be in the default namespace.
			§ policies=static-web-policy: The role grants the static-web-policy.
			§ ttl=1h: Tokens obtained via this role are valid for 1 hour.
		○ Actual Step 14 (Run the role creation script): 
Bash

bash C:/projects/aks-k8s-project/vault-policies/static-web-role.sh
	• Actual Step 15 (Create Kubernetes Service Account for your app): 
		○ Create C:/projects/aks-k8s-project/static-web/templates/serviceaccount.yaml: 
YAML

apiVersion: v1
kind: ServiceAccount
metadata:
  name: static-web-sa
  namespace: default
		○ Actual Step 16 (Update Deployment to use Service Account): 
			§ Modify C:/projects/aks-k8s-project/static-web/templates/deployment.yaml to include serviceAccountName: 
YAML

# ... existing deployment YAML ...
      serviceAccountName: static-web-sa # Add this line under spec.template.spec
# ... rest of deployment YAML ...
	• Actual Step 17 (Install Vault Agent Sidecar (optional but recommended for secret injection)): 
		○ While applications can directly interact with Vault, the Vault Agent sidecar is often preferred for injecting secrets into pods.
		○ Actual Step 18 (Redeploy Helm Chart to apply Service Account and prepare for secret injection): 
Bash

cd C:/projects/aks-k8s-project
helm upgrade --install static-web ./static-web --namespace default
			§ Explanation: This redeploys your static-web application, now configured to use the static-web-sa service account.
✅ Checkpoint: Vault HA running, initialized, secrets configured, and app service account ready.
	• Verification: 
		○ kubectl get pods -l app.kubernetes.io/name=vault shows all Vault pods are Running.
		○ kubectl exec -it vault-0 -- vault status shows Sealed: false.
		○ Confirm your vault-init.json file exists and contains the unseal keys and root token.
		○ Confirm the static-web-sa service account exists: kubectl get sa static-web-sa -n default.

📦 6️⃣️⃣ ARGOCd GITOPS SETUP
📂 Files:
	• argocd-app.yaml (Will create later)
⚙ Why: ArgoCD is a declarative, GitOps continuous delivery tool for Kubernetes. It automates the deployment of applications to your cluster by continuously syncing your Git repository (source of truth) with the desired state of your applications in the cluster.
📜 Commands [VS Code Terminal, Bash]:
	• Actual Step 1 (Create ArgoCD namespace): 
Bash

kubectl create namespace argocd
	• Actual Step 2 (Apply ArgoCD manifests): 
Bash

kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
		○ Explanation: This command downloads and applies all the necessary Kubernetes resources (Deployments, Services, RBAC roles, etc.) to install ArgoCD in the argocd namespace.
	• Actual Step 3 (Wait for ArgoCD pods to be ready): 
Bash

kubectl get pods -n argocd

Wait until all ArgoCD pods are in a Running or Completed state.
	• Actual Step 4 (Port-forward ArgoCD server): 
Bash

kubectl port-forward svc/argocd-server -n argocd 8080:443
		○ Explanation: This command forwards traffic from your local port 8080 to the ArgoCD server's HTTPS port (443) inside the cluster. Keep this command running in a dedicated terminal.
	• Actual Step 5 (Access ArgoCD UI): Open your web browser and go to https://localhost:8080. You will likely encounter a certificate warning; proceed past it.
	• Actual Step 6 (Get initial admin password): Open a new VS Code terminal (Bash). 
Bash

kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d; echo
		○ Explanation: Retrieves the auto-generated initial admin password for the ArgoCD UI. base64 -d decodes it.
	• Actual Step 7 (Login to ArgoCD UI): Use admin as the username and the password obtained in the previous step.
	• Actual Step 8 (Create argocd-app.yaml for your static website): 
		○ Create the file C:/projects/aks-k8s-project/argocd-app.yaml: 
YAML

apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: static-web-app
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/<YOUR_GITHUB_USERNAME>/aks-k8s-project.git # ⚠️ EDIT: Your GitHub repo URL
    targetRevision: HEAD
    path: static-web # Path to your Helm chart within the repo
  destination:
    server: https://kubernetes.default.svc
    namespace: default # The namespace where your app is deployed
  syncPolicy:
    automated:        # Enables automated sync
      prune: true     # Allows ArgoCD to delete resources not in Git
      selfHeal: true  # Automatically syncs if live state deviates from Git
    syncOptions:
    - CreateNamespace=true # Ensure the namespace exists if not already
		○ ⚠️ EDIT: You MUST replace https://github.com/<YOUR_GITHUB_USERNAME>/aks-k8s-project.git with the actual URL of your GitHub repository after you have pushed your project files to it. This is a crucial step for GitOps. For now, you can use a dummy URL, but remember to update it.
		○ Actual Step 9 (Push your project to GitHub): 
			§ Initialize a Git repository in C:/projects/aks-k8s-project.
			§ Create a new repository on GitHub (e.g., aks-k8s-project).
			§ Follow the GitHub instructions to link your local repo and push. 
Bash

cd C:/projects/aks-k8s-project
git init
git add .
git commit -m "Initial project commit"
git branch -M main
git remote add origin https://github.com/<YOUR_GITHUB_USERNAME>/aks-k8s-project.git # ⚠️ EDIT
git push -u origin main
	• Actual Step 10 (Create ArgoCD Application): 
Bash

kubectl apply -n argocd -f C:/projects/aks-k8s-project/argocd-app.yaml
		○ Explanation: This command tells ArgoCD about your application and where its source code (Helm chart) is located in Git. ArgoCD will then start monitoring that repository and synchronize the cluster state accordingly.
✅ Checkpoint: ArgoCD installed + UI accessible, and application defined.
	• Verification: 
		○ You can log in to the ArgoCD UI at https://localhost:8080.
		○ You should see your static-web-app application listed. It might initially show as OutOfSync or Progressing and then Synced. Click into it to see the Kubernetes resources managed by ArgoCD.
		○ Ensure your project files are pushed to the GitHub repository you specified in argocd-app.yaml.

🛡 7️⃣️⃣ SNYK SECURITY SCANNING
⚙ Why: Snyk is a developer security platform that helps find and fix vulnerabilities in your code, open source dependencies, containers, and infrastructure as code. Checking images early in the pipeline helps identify and remediate security risks before deployment.
📜 Commands [PowerShell]:
	• Actual Step 1 (Install Snyk CLI): Open PowerShell. 
PowerShell

npm install -g snyk
		○ Explanation: Installs the Snyk command-line interface globally using Node Package Manager (npm).
	• Actual Step 2 (Authenticate Snyk): 
PowerShell

snyk auth
		○ Explanation: This will open a browser window and prompt you to log in to your Snyk account or create one. Once authenticated, it will link your CLI to your Snyk account.
	• Actual Step 3 (Scan your Docker image): 
PowerShell

snyk test --docker static-web:v1
		○ Explanation: snyk test performs a vulnerability scan. --docker static-web:v1 specifies that you want to scan the locally built Docker image static-web:v1.
		○ Actual Step 4 (View results): The command will output a report of any vulnerabilities found, including severity and suggested fixes.
✅ Checkpoint: Image scanned.
	• Verification: Confirm that the snyk test command ran successfully and provided a vulnerability report (even if it says "No vulnerabilities found").

📊 8️⃣️⃣ MONITORING: PROMETHEUS + GRAFANA + AZURE MONITOR
⚙ Why: Comprehensive monitoring is essential for understanding the health and performance of your applications and infrastructure. Prometheus collects metrics, Grafana visualizes them, and Azure Monitor provides unified monitoring across your Azure resources.
📜 Commands [VS Code Terminal, Bash]:
	• Actual Step 1 (Add Prometheus Community Helm repo): Open VS Code terminal (Bash). 
Bash

helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
	• Actual Step 2 (Install kube-prometheus-stack): This chart installs Prometheus, Grafana, Alertmanager, and other monitoring components. 
Bash

helm install kube-prometheus-stack prometheus-community/kube-prometheus-stack --namespace monitoring --create-namespace
		○ Explanation: Installs the kube-prometheus-stack chart into a new namespace called monitoring.
	• Actual Step 3 (Wait for pods to be ready): 
Bash

kubectl get pods -n monitoring

Wait until all Prometheus, Grafana, and other monitoring pods are Running. This can take several minutes.
	• Actual Step 4 (Port-forward Grafana): 
Bash

kubectl port-forward svc/kube-prometheus-stack-grafana -n monitoring 3000:80
		○ Explanation: Forwards local port 3000 to Grafana's service port 80. Keep this running in a dedicated terminal.
	• Actual Step 5 (Access Grafana UI): Open your web browser and go to http://localhost:3000.
	• Actual Step 6 (Get Grafana admin password): In a new VS Code terminal (Bash): 
Bash

kubectl get secret -n monitoring kube-prometheus-stack-grafana -o jsonpath="{.data.admin-password}" | base64 -d; echo
		○ Explanation: Retrieves the auto-generated admin password for Grafana.
	• Actual Step 7 (Login to Grafana UI): Use admin as the username and the password obtained in the previous step. Explore the pre-configured Kubernetes dashboards.
📜 Azure Monitor [PowerShell]:
	• Actual Step 1 (Enable Azure Monitor add-on for AKS): Open PowerShell. 
PowerShell

az aks enable-addons --addons monitoring --name myaks --resource-group aks-rg
		○ Explanation: This command enables the Azure Monitor (Container Insights) add-on for your AKS cluster. It deploys the necessary agents and configurations to send logs and metrics from your AKS cluster to Azure Monitor.
		○ Verification: Go to the Azure portal, navigate to your AKS cluster (myaks), and click on Insights under Monitoring. You should see performance data and logs starting to populate.
✅ Checkpoint: Monitoring dashboards running.
	• Verification: 
		○ You can access Grafana at http://localhost:3000 and log in.
		○ You can view metrics and logs for your AKS cluster in the Azure portal under Insights for your AKS resource.

🔄 9️⃣️⃣ REDEPLOY PIPELINE WITH JENKINS
📂 Files:
	• Jenkinsfile (Will create later)
⚙ Why: Jenkins is a popular open-source automation server that enables continuous integration and continuous delivery (CI/CD). By defining a Jenkinsfile in your repository, you automate the entire process from code commit to deployment, ensuring consistency and speed.
Actual Steps (Detailed Jenkins Setup):
	• Actual Step 1 (Install Jenkins on a VM or Docker): For this guide, we'll suggest a basic Docker setup for simplicity, but a production Jenkins would run on a dedicated VM or as part of a larger CI/CD infrastructure. 
		○ Option A: Run Jenkins in Docker (simplest for local testing): 
PowerShell

docker run -d -p 8080:8080 -p 50000:50000 --name jenkins --restart=on-failure -v C:/jenkins_home:/var/jenkins_home jenkins/jenkins:lts-jdk11
			§ Explanation: Runs a Jenkins LTS container, mapping ports 8080 (web UI) and 50000 (agent communication). -v C:/jenkins_home:/var/jenkins_home persists Jenkins data on your local machine.
		○ Option B: Install Jenkins on an Azure VM: (More realistic for production, but involves more setup, including networking). 
			§ Create a Linux VM in Azure.
			§ SSH into the VM.
			§ Install Java: sudo apt update && sudo apt install openjdk-11-jdk -y
			§ Add Jenkins apt repository: 
Bash

sudo wget -O /usr/share/keyrings/jenkins-keyring.asc \
  https://pkg.jenkins.io/debian-stable/jenkins.io-2023.key
echo "deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc] \
  https://pkg.jenkins.io/debian-stable binary/" | sudo tee \
  /etc/apt/sources.list.d/jenkins.list > /dev/null
sudo apt-get update
sudo apt-get install jenkins -y
			§ Start Jenkins: sudo systemctl start jenkins
			§ Enable on boot: sudo systemctl enable jenkins
			§ Open port 8080 on the VM's network security group.
	• Actual Step 2 (Access Jenkins UI): 
		○ If Docker: Go to http://localhost:8080.
		○ If VM: Go to http://<VM_PUBLIC_IP>:8080.
	• Actual Step 3 (Unlock Jenkins): 
		○ Get the initial admin password: 
			§ If Docker: docker logs jenkins (look for "initialAdminPassword").
			§ If VM: sudo cat /var/lib/jenkins/secrets/initialAdminPassword.
		○ Enter the password in the UI and click Continue.
	• Actual Step 4 (Install Plugins): 
		○ Select Install suggested plugins.
		○ Wait for plugins to install.
	• Actual Step 5 (Create Admin User): 
		○ Create your first admin user.
	• Actual Step 6 (Install necessary Jenkins Plugins for this project): 
		○ Go to Manage Jenkins > Plugins > Available plugins.
		○ Search and install: Docker Pipeline, Kubernetes, Azure Container Registry (ACR).
		○ Restart Jenkins if prompted.
	• Actual Step 7 (Configure Jenkins Credentials): 
		○ You'll need credentials for Azure CLI and GitHub in Jenkins.
		○ Azure Credentials: 
			§ Create an Azure Service Principal: 
PowerShell

az ad sp create-for-rbac --name http://jenkins-sp --role contributor --scopes /subscriptions/<YOUR_SUBSCRIPTION_ID> --json-output
				□ ⚠️ EDIT: Replace <YOUR_SUBSCRIPTION_ID>. Save the appId, password, and tenant from the output.
			§ In Jenkins, go to Manage Jenkins > Credentials > System > Global credentials (unrestricted) > Add Credentials.
			§ Kind: Azure Service Principal
			§ Scope: Global
			§ Subscription ID: Your Azure subscription ID
			§ Client ID: appId from SP output
			§ Client Secret: password from SP output
			§ Tenant ID: tenant from SP output
			§ ID: azure-credentials (or any unique ID)
			§ Description: Azure Service Principal for Jenkins
			§ Click Create.
		○ GitHub Credentials: 
			§ Generate a GitHub Personal Access Token (PAT) with repo scope: https://github.com/settings/tokens.
			§ In Jenkins, Manage Jenkins > Credentials > System > Global credentials (unrestricted) > Add Credentials.
			§ Kind: Secret text
			§ Scope: Global
			§ Secret: Your GitHub PAT
			§ ID: github-pat (or any unique ID)
			§ Description: GitHub PAT for Jenkins
			§ Click Create.
	• Actual Step 8 (Configure Jenkins with kubectl and Helm): 
		○ Jenkins needs to interact with your AKS cluster. The easiest way is to use a Jenkins agent running in Kubernetes or by configuring kubectl/Helm on the Jenkins host.
		○ Option A (Simplest for this guide - on Jenkins host): 
			§ If Jenkins is on a Docker container, you'll need to mount the kubeconfig: -v C:/Users/<YourUser>/.kube:/var/jenkins_home/.kube to the docker run command for Jenkins. Re-run your docker run command with this addition.
			§ You'll also need kubectl and helm binaries inside the Jenkins container. You can create a custom Docker image for Jenkins with these pre-installed or use docker exec to install them after the container starts. This adds complexity.
		○ Option B (Recommended - Kubernetes Plugin and JCasC): Install the Kubernetes plugin in Jenkins. Configure cloud agents that run as pods within your AKS cluster. This is the most robust solution for Jenkins on Kubernetes. This is a larger topic, so for a "foolproof" guide, we'll stick to a simpler approach (local Jenkins for demo).
		○ For this project, we'll assume Jenkins has access to kubectl and Helm. If using the Docker Jenkins, you'd likely need to volume mount your host's .kube config and potentially exec into the container to install kubectl and helm or build a custom Jenkins image. For the sake of this detailed guide, let's assume kubectl and helm are available in the Jenkins environment where the pipeline runs.
	• Actual Step 9 (Create a Jenkinsfile): 
		○ In your C:/projects/aks-k8s-project directory, create a Jenkinsfile (no extension).
		○ C:/projects/aks-k8s-project/Jenkinsfile (Example): 
Groovy

pipeline {
    agent any

environment {
        RESOURCE_GROUP = "aks-rg"
        CLUSTER_NAME   = "myaks"
        ACR_NAME       = "myacrname" // ⚠️ EDIT: Your ACR name
        IMAGE_NAME     = "static-web"
        IMAGE_TAG      = "v1"
        CHART_PATH     = "static-web" // Path to your Helm chart within the repo
        KUBECONFIG_FILE = "${HOME}/.kube/config" // Path to kubeconfig in Jenkins home
    }

stages {
        stage('Git Checkout') {
            steps {
                git branch: 'main', credentialsId: 'github-pat', url: 'https://github.com/<YOUR_GITHUB_USERNAME>/aks-k8s-project.git' // ⚠️ EDIT: Your GitHub repo URL and credentialsId
            }
        }

stage('Terraform Apply') {
            steps {
                script {
                    // Ensure Terraform is installed and Azure CLI is logged in
                    // For a real Jenkins, you'd have Terraform binary pre-installed on the agent
                    // and AZ CLI configured via service principal (see Jenkins credentials)
                    withCredentials([azureServicePrincipal('azure-credentials')]) { // Use the Azure credential ID
                        sh """
                            # Ensure Azure CLI is logged in (optional if using Azure Credential for tasks)
                            az login --service-principal -u $AZURE_CLIENT_ID -p $AZURE_CLIENT_SECRET --tenant <span class="math-inline">AZURE\_TENANT\_ID
# Change to terraform directory cd terraform # Initialize Terraform terraform init # Apply Terraform (consider terraform plan -out=tfplan and terraform apply tfplan for safer prod) terraform apply -auto-approve \\ -var="resource_group={RESOURCE_GROUP}" \ -var="location=uksouth" \ -var="cluster_name=CLUSTERN​AME"−var="dnsp​refix={CLUSTER_NAME}dns" \ -var="node_count=2" """ } } } }
            stage('Docker Build and Push') {
                steps {
                    script {
                        withCredentials([azureServicePrincipal('azure-credentials')]) { // Use the Azure credential ID
                            sh """
                                # Login to Azure Container Registry
                                az acr login -n ${ACR_NAME}
# Change to website directory
                                cd website
# Build Docker image
                                docker build -t <span class="math-inline">\{ACR\_NAME\}\.azurecr\.io/</span>{IMAGE_NAME}:${IMAGE_TAG} .
# Push Docker image to ACR
                                docker push <span class="math-inline">\{ACR\_NAME\}\.azurecr\.io/</span>{IMAGE_NAME}:${IMAGE_TAG}
                            """
                        }
                    }
                }
            }
stage('Helm Deploy') {
                steps {
                    script {
                        // Fetch kubeconfig from AKS and save it locally for Helm/kubectl
                        withCredentials([azureServicePrincipal('azure-credentials')]) {
                            sh """
                                az login --service-principal -u $AZURE_CLIENT_ID -p $AZURE_CLIENT_SECRET --tenant <span class="math-inline">AZURE\_TENANT\_ID
# Create .kube directory if it doesn't exist mkdir -p \(dirname ${KUBECONFIG_FILE}) # Get AKS credentials and save to default kubeconfig location az aks get-credentials --resource-group ${RESOURCE_GROUP} --name ${CLUSTER_NAME} --file ${KUBECONFIG_FILE} --overwrite-cli """ } // Helm deployment sh """ # Change to project root directory to run Helm from there cd ${WORKSPACE} # Upgrade/Install Helm chart helm upgrade --install IMAGEN​AME./{CHART_PATH} --namespace default """ } } }
            stage('ArgoCD Sync') {
                steps {
                    script {
                        // Ensure kubectl is configured
                        sh """
                            # Ensure kubectl is configured with the right context
                            # This is implicitly handled by the previous stage's az aks get-credentials,
                            # but explicitly setting KUBECONFIG for shell commands is good practice.
                            export KUBECONFIG=<span class="math-inline">\{KUBECONFIG\_FILE\}
# Wait for ArgoCD server to be ready (optional, but good for robust pipeline) kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=argocd-server -n argocd --timeout=300s || true # Sync ArgoCD application # First, get the ArgoCD CLI password ARGO_PASSWORD=\(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d)
                            # Login to ArgoCD CLI (requires ArgoCD CLI to be installed on Jenkins agent)
                            # For a real Jenkins, you'd install argocd CLI on the agent.
                            # argocd login localhost:8080 --username admin --password \$ARGO_PASSWORD --insecure
# Instead of CLI login, we can use kubectl to trigger sync via resource patching
                            kubectl patch application static-web-app -n argocd --type merge -p '{"spec":{"syncPolicy":{"automated":{"prune":true,"selfHeal":true}}}}' || true # Re-enable automated sync if it was disabled
                            kubectl patch application static-web-app -n argocd --type merge -p '{"status":{"operations":{"sync":{"phase":"Succeeded"}}}}' || true # Clear previous sync state
                            kubectl annotate application static-web-app argocd.argoproj.io/refresh=now -n argocd --overwrite
                            kubectl wait --for=jsonpath='{.status.sync.status}'=Synced application static-web-app -n argocd --timeout=600s || true
                        """
                    }
                }
            }
        }
    }
    ```
    * **⚠️ IMPORTANT EDITS for Jenkinsfile:**
        * `repoURL`: Update `https://github.com/<YOUR_GITHUB_USERNAME>/aks-k8s-project.git` with your actual GitHub repository URL.
        * `credentialsId: 'github-pat'`: Ensure `github-pat` matches the ID of your GitHub PAT credential in Jenkins.
        * `ACR_NAME`: Set to your actual ACR name.
        * `azureServicePrincipal('azure-credentials')`: Ensure `azure-credentials` matches the ID of your Azure Service Principal credential in Jenkins.
        * **`KUBECONFIG_FILE` path:** The path `${HOME}/.kube/config` assumes Jenkins runs as a user and has a home directory. Adjust if Jenkins is configured differently. For Docker, if you mapped `C:/Users/<YourUser>/.kube` to `/var/jenkins_home/.kube`, then `${HOME}` would be `/var/jenkins_home`.
        * **Terraform/Docker/kubectl/Helm binaries:** The `Jenkinsfile` assumes these tools are available in the `PATH` of the Jenkins agent executing the pipeline. For a Dockerized Jenkins, you'd need to either build a custom Docker image including these tools or install them on the fly in the pipeline (which is less robust). For a VM, ensure they are installed on the VM.
        * **ArgoCD CLI:** The Jenkinsfile includes commands to interact with ArgoCD. You'll need the `argocd` CLI tool installed on the Jenkins agent. For the `kubectl patch` method, it's sufficient if `kubectl` is present.
	• Actual Step 10 (Commit and Push Jenkinsfile): 
Bash

cd C:/projects/aks-k8s-project
git add Jenkinsfile
git commit -m "Add Jenkinsfile"
git push origin main
	• Actual Step 11 (Create Jenkins Pipeline Job): 
		○ In Jenkins UI, click New Item.
		○ Enter an item name (e.g., aks-k8s-deployment).
		○ Select Pipeline. Click OK.
		○ In the configuration, go to Pipeline section.
		○ Definition: Pipeline script from SCM.
		○ SCM: Git.
		○ Repository URL: https://github.com/<YOUR_GITHUB_USERNAME>/aks-k8s-project.git (your GitHub repo).
		○ Credentials: Select your github-pat credential.
		○ Branches to build: */main.
		○ Script Path: Jenkinsfile (default).
		○ Click Save.
	• Actual Step 12 (Run the Jenkins Pipeline): 
		○ Click Build Now on your new pipeline job.
		○ Monitor the build progress. You should see it execute each stage defined in your Jenkinsfile.
✅ Checkpoint: CI/CD full loop working.
	• Verification: 
		○ Jenkins job completes successfully.
		○ You can make a small change to your index.html (e.g., change "Welcome to AKS!" to "Hello from Jenkins!"), commit and push to GitHub.
		○ Jenkins should automatically trigger (if webhook is configured, or you trigger manually).
		○ The static-web application in AKS should redeploy, and you should see your changes live on the LoadBalancer IP.
		○ ArgoCD UI should show the static-web-app as Synced.

🧭 NAVIGATION SUMMARY
	• VS Code Terminal (Bash): Terraform, Helm, kubectl, Vault (via kubectl exec), ArgoCD (via kubectl apply/patch).
	• PowerShell: Docker build/run/tag/push, Azure CLI (login, ACR, AKS get-credentials, enable add-ons), Snyk.
	• Browser UIs: ArgoCD, Grafana, Jenkins, Azure Portal.
	• Save all configs + scripts inside C:/projects/aks-k8s-project: This ensures everything is version-controlled and self-contained.
✅ All steps are now complete, foolproof, and ready.

GO PACKAGE!
Here's how you could structure a simple package/repo (not a full ZIP, but the conceptual layout for GitHub):
aks-k8s-project/
├── .gitignore
├── Jenkinsfile
├── argocd-app.yaml
├── README.md
├── terraform/
│   ├── main.tf
│   ├── variables.tf
│   ├── outputs.tf
│   └── modules/
│       └── aks/
│           ├── main.tf
│           ├── variables.tf
│           └── outputs.tf
├── static-web/
│   ├── Chart.yaml
│   ├── values.yaml
│   └── templates/
│       ├── deployment.yaml
│       ├── service.yaml
│       └── serviceaccount.yaml # New for Vault integration
├── website/
│   ├── index.html
│   ├── Dockerfile
│   └── (other static assets like css, js, images)
├── vault-policies/
│   ├── static-web-policy.hcl
│   └── static-web-role.sh
└── vault-init.json # This should NOT be committed to Git in a real scenario due to sensitive data!

This expanded guide should provide significantly more detail and actionable steps for someone to follow the project from start to finish. Remember to replace placeholders like <YOUR_GITHUB_USERNAME> and myacrname with your actual values.
